PLAN OF ATTACK

Cross-Validation Approach:
- Nested k-fold cross-validation 
	- no need for test set

Hyperparameter Tuning:
- Gridsearch

Models:
- SVC (standardize)
- Random Forest (no need to standardize)
	- first attempt: 1000 trees
- KNN (standardize)
- XGBoost
- AdaBoost

Model processing:
- Pipeline

Preprocessing:

Phase 1:
- delete:
	- redundant columns
	- meaningless columns
	- columns with too many values
	- columns with too many nulls

- transform:
	- impute missing values using appropriate strategy
	- convert categorical variables into dummy variables
	- datime into year column and month column
	- datetime to an ordinal value

Phase 2:
- transform:
	- dedupe columns with more than 90% correlation
	- remove columns with sum less than 30?
	- remove columns with class distribution identical to overall population?


Notes:
- when does the curse of dimensionality kick in? (add to data science dictionary)
	- done
- Need to understand the fillna by groups process better:
    data = X_train.groupby(['district_code']).longitude
    X_train['longitude'] = data.transform(lambda x: x.fillna(x.mean()))

    med = X_train.groupby('district_code')['latitude'].transform('mean')
    X_train['latitude'].fillna(med)

    fill_mean = lambda g: g.fillna(g.mean())
    X_train['gps_height'] = X_train.groupby('district_code').gps_height.apply(fill_mean)

	fill_values = dict(X_train['longitude'].groupby(X_train['district_code']).mean())

	fill_func = lambda g: g.fillna(fill_values[g.name])
	X_train['longitude'] = X_train['longitude'].groupby(X_train['district_code']).apply(fill_func)
- it appears that with RF, cross-validation may in fact be better than the OOB score. With 100 trees, OOB score was .8054, CV score was 0.7977, adn true score was 0.7907
- appears that a strong correlation between coefficients causes issues in RF. removing repetitive columns improves model performance by a lot.
- need to save every dataset, every model, and every submission as a pickle, and map each one to each other.
- pd.to_pickle results in file sizes that are twice as big as pd.to_csv
	- might need to research SQLite databases instead
- a manual test of different min_samples_split sizes (GridSearchCV broke down for some reason) showed that the results were same for n=2, 50, and 100
- imputation techniques matter!
- is more trees always better (or is there a negative testing effect of overfitting)?
	- when using oob score, it seemed that more trees is not necessarily better (that there's an ideal number), but in practice, more trees always validated better
- need to name my submission files with more details
- when doing the whole process - need to be able to keep track of everything:
	- need to know why you made every decision that you made
	- need to be able to reproduce every single model that you ever created
	- this will result in a lot of code and notes, but it will all eventually be archived
		- only the final model and the clean version need to be presented (but again, with clear documentation of that whole process).

Results:
1. RF, 1000 trees (OOB score .8032): 0.7813
2. RF, 100 trees (OOB score .8030): 0.7812
3. RF, 100 trees, remove dummies with less than 100 values (OOB score .79): 0.7701
4. RF, 100 trees, keep ALL dummies (OOB score .8054): 0.7907
5. RF, 1000 trees, keep ALL dummies (OOB score .8076): 0.7935
6. RF, 500 trees, keeps ALL dummies, remove correlated columns (OOB score .8108; CV score 0.8034): 0.7966
7. RF, 1000 trees, keeps ALL dummies, remove correlated columns, fix imputation (OOB score .8083; CV score 0.8001): 0.7927
8. RF, 500 trees, keeps ALL dummies, fix imputation (OOB score .8080; CV score 0.8004): 0.8002
9. RF, 1000 trees, keeps ALL dummies, fix imputation, remove correlation (OOB score .8080; CV score 0.8004): 0.8002
10. RF, 1000 trees, keeps ALL dummies, fix imputation (OOB score .8080; CV score 0.8004): 0.8005
11. RF, 1000 trees, construction2, gini, min_samples_split = 12 (OOB score .8142): 0.7919
12. RF, 1000 trees, construction2, gini (OOB score .8142): 0.7946
13. RF, 1000 trees, construction2 (OOB score ~.807): 0.7952
14. R4, y_test1 (OOB score 0.8092; CV score 0.8027): 0.7947
15. RF, y_test3 (OOB score 0.8142; CV score 0.8072): 0.8002
16. RF, y_test4 (OOB score 0.8151; CV score 0.8072): 0.7921
17. RF, y_test7 (OOB score 0.8150; CV score 0.8075): 0.7936
18. RF, y_test6 (OOB score 0.8138; CV score 0.8065): 0.7985
19. RF, y_test8 (OOB score 0.8144: CV score 0.8072): 0.8019

Proper workflow:
1. File that loads the data into directory
2. EDA & preprocessing file:
	- understand the data, and depending on the different theories and models, generate different datasets and save as csv (make sure that each one is clearly explained).
3. For each kind of model, create new notebook:
	- load cleaned datasets that apply to this model
	- run model with CV, generate score, and pickle that model
	- run model diagnostics if necessary
	- for selected models, generate submission file