PLAN OF ATTACK

Cross-Validation Approach:
- Nested k-fold cross-validation 
	- no need for test set

Hyperparameter Tuning:
- Gridsearch

Models:
- SVC (standardize)
	- 
- Random Forest (no need to standardize)
	- first attempt: 1000 trees
- KNN (standardize)
- Ensemble method
- AdaBoost

Model processing:
- Pipeline

Preprocessing:

- Each variable should be categorized as:
	- Keep as is:
		- Keep numeric columns with no nulls
		- Keep categorical columns with 10 or fewer unique values
			- Transform into dummy variables
	- Drop:
		- Drop anything with more than 20% nulls
		- Drop meaningless columns (such as ID)
	- Transform numeric columns:
		- Impute missing values (check for zeros)
		- Change date columns to months (or quarters)
			- group like months together based on functionality
	- Transform categorical columns:
		- Combine categories based on functionality (min 70%), non-functionality (min 50%), needs repair (min 20%), or mixed. If new value from test set, set to mixed.
Research:
- when does the curse of dimensionality kick in? (add to data science dictionary)
- Need to understand the fillna by groups process better:
    data = X_train.groupby(['district_code']).longitude
    X_train['longitude'] = data.transform(lambda x: x.fillna(x.mean()))

    med = X_train.groupby('district_code')['latitude'].transform('mean')
    X_train['latitude'].fillna(med)

    fill_mean = lambda g: g.fillna(g.mean())
    X_train['gps_height'] = X_train.groupby('district_code').gps_height.apply(fill_mean)

	fill_values = dict(X_train['longitude'].groupby(X_train['district_code']).mean())

	fill_func = lambda g: g.fillna(fill_values[g.name])
	X_train['longitude'] = X_train['longitude'].groupby(X_train['district_code']).apply(fill_func)
- it appears that with RF, cross-validation may in fact be better than the OOB score. With 100 trees, OOB score was .8054, CV score was 0.7977, adn true score was 0.7907
- appears that a strong correlation between coefficients causes issues in RF. removing repetitive columns improves model performance by a lot.
- need to save every dataset, every model, and every submission as a pickle, and map each one to each other.

Results:
1. RF, 1000 trees (OOB score .8032): 0.7813
2. RF, 100 trees (OOB score .8030): 0.7812
3. RF, 100 trees, remove dummies with less than 100 values (OOB score .79): 0.7701
4. RF, 100 trees, keep ALL dummies (OOB score .8054): 0.7907
5. RF, 1000 trees, keep ALL dummies (OOB score .8076): 0.7935
6. RF, 500 trees, keeps ALL dummies, remove correlated columns (OOB score .8108; CV score 0.8034): 0.7966
7. RF, 1000 trees, keeps ALL dummies, remove correlated columns, fix imputation (OOB score .8083; CV score 0.8001): 0.7927
8. RF, 500 trees, keeps ALL dummies, fix imputation (OOB score .8083; CV score 0.8001): 0.7927

Proper workflow:
1. File that loads the data into directory
2. EDA & preprocessing file:
	- understand the data, and depending on the different theories and models, generate different datasets and save as csv (make sure that each one is clearly explained).
3. For each kind of model, create new notebook:
	- load cleaned datasets that apply to this model
	- run model with CV, generate score, and pickle that model
	- run model diagnostics if necessary
	- for selected models, generate submission file