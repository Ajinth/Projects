PLAN OF ATTACK

Cross-Validation Approach:
- Nested k-fold cross-validation 
	- no need for test set

Hyperparameter Tuning:
- Gridsearch

Models:
- SVC (standardize)
- Random Forest (no need to standardize)
	- first attempt: 1000 trees
- KNN (standardize)
- XGBoost
- AdaBoost

Model processing:
- Pipeline

Preprocessing:

Phase 1:
- delete:
	- redundant columns
	- meaningless columns
	- columns with too many values
	- columns with too many nulls

- transform:
	- impute missing values using appropriate strategy
	- convert categorical variables into dummy variables
	- datime into year column and month column
	- datetime to an ordinal value

Phase 2:
- transform:
	- dedupe columns with more than 90% correlation
	- remove columns with sum less than 30?
	- remove columns with class distribution identical to overall population?


Notes:
- when does the curse of dimensionality kick in? (add to data science dictionary)
- Need to understand the fillna by groups process better:
    data = X_train.groupby(['district_code']).longitude
    X_train['longitude'] = data.transform(lambda x: x.fillna(x.mean()))

    med = X_train.groupby('district_code')['latitude'].transform('mean')
    X_train['latitude'].fillna(med)

    fill_mean = lambda g: g.fillna(g.mean())
    X_train['gps_height'] = X_train.groupby('district_code').gps_height.apply(fill_mean)

	fill_values = dict(X_train['longitude'].groupby(X_train['district_code']).mean())

	fill_func = lambda g: g.fillna(fill_values[g.name])
	X_train['longitude'] = X_train['longitude'].groupby(X_train['district_code']).apply(fill_func)
- it appears that with RF, cross-validation may in fact be better than the OOB score. With 100 trees, OOB score was .8054, CV score was 0.7977, adn true score was 0.7907
- appears that a strong correlation between coefficients causes issues in RF. removing repetitive columns improves model performance by a lot.
- need to save every dataset, every model, and every submission as a pickle, and map each one to each other.
- pd.to_pickle results in file sizes that are twice as big as pd.to_csv
	- might need to research SQLite databases instead
- a manual test of different min_samples_split sizes (GridSearchCV broke down for some reason) showed that the results were same for n=2, 50, and 100

Results:
1. RF, 1000 trees (OOB score .8032): 0.7813
2. RF, 100 trees (OOB score .8030): 0.7812
3. RF, 100 trees, remove dummies with less than 100 values (OOB score .79): 0.7701
4. RF, 100 trees, keep ALL dummies (OOB score .8054): 0.7907
5. RF, 1000 trees, keep ALL dummies (OOB score .8076): 0.7935
6. RF, 500 trees, keeps ALL dummies, remove correlated columns (OOB score .8108; CV score 0.8034): 0.7966
7. RF, 1000 trees, keeps ALL dummies, remove correlated columns, fix imputation (OOB score .8083; CV score 0.8001): 0.7927
8. RF, 500 trees, keeps ALL dummies, fix imputation (OOB score .8083; CV score 0.8001): 0.7927
9. RF, 1000 trees, keeps ALL dummies, fix imputation (OOB score .8080; CV score 0.8004): 0.8002

Proper workflow:
1. File that loads the data into directory
2. EDA & preprocessing file:
	- understand the data, and depending on the different theories and models, generate different datasets and save as csv (make sure that each one is clearly explained).
3. For each kind of model, create new notebook:
	- load cleaned datasets that apply to this model
	- run model with CV, generate score, and pickle that model
	- run model diagnostics if necessary
	- for selected models, generate submission file